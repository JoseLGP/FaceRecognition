{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Process for testing FaceNet \n",
    "\n",
    "# Face detection\n",
    "\n",
    "For installing MTCNN face detector:\n",
    "\t\n",
    "-sudo pip install mtcnn (Linux)\n",
    "\n",
    "-pip install mtcnn (windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "# confirm mtcnn was installed correctly\n",
    "import mtcnn\n",
    "# print version\n",
    "print(mtcnn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detection for the 5 Celebrity Faces Dataset\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    " \n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "\t# load image from file\n",
    "\timage = Image.open(filename)\n",
    "\t# convert to RGB, if needed\n",
    "\timage = image.convert('RGB')\n",
    "\t# convert to array\n",
    "\tpixels = asarray(image)\n",
    "\t# create the detector, using default weights\n",
    "\tdetector = MTCNN()\n",
    "\t# detect faces in the image\n",
    "\tresults = detector.detect_faces(pixels)\n",
    "\t# extract the bounding box from the first face\n",
    "\tx1, y1, width, height = results[0]['box']\n",
    "\t# bug fix\n",
    "\tx1, y1 = abs(x1), abs(y1)\n",
    "\tx2, y2 = x1 + width, y1 + height\n",
    "\t# extract the face\n",
    "\tface = pixels[y1:y2, x1:x2]\n",
    "\t# resize pixels to the model size\n",
    "\timage = Image.fromarray(face)\n",
    "\timage = image.resize(required_size)\n",
    "\tface_array = asarray(image)\n",
    "\treturn face_array\n",
    " \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "\tfaces = list()\n",
    "\t# enumerate files\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + filename\n",
    "\t\t# get face\n",
    "\t\tface = extract_face(path)\n",
    "\t\t# store\n",
    "\t\tfaces.append(face)\n",
    "\treturn faces\n",
    " \n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "\tX, y = list(), list()\n",
    "\t# enumerate folders, on per class\n",
    "\tfor subdir in listdir(directory):\n",
    "\t\t# path\n",
    "\t\tpath = directory + subdir + '/'\n",
    "\t\t# skip any files that might be in the dir\n",
    "\t\tif not isdir(path):\n",
    "\t\t\tcontinue\n",
    "\t\t# load all faces in the subdirectory\n",
    "\t\tfaces = load_faces(path)\n",
    "\t\t# create labels\n",
    "\t\tlabels = [subdir for _ in range(len(faces))]\n",
    "\t\t# summarize progress\n",
    "\t\tprint('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "\t\t# store\n",
    "\t\tX.extend(faces)\n",
    "\t\ty.extend(labels)\n",
    "\treturn asarray(X), asarray(y)\n",
    " \n",
    "# load train dataset\n",
    "trainX, trainy = load_dataset('5-celebrity-faces-dataset/train/')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "testX, testy = load_dataset('5-celebrity-faces-dataset/val/')\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('5-celebrity-faces-dataset.npz', trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process has to be repeated for all datasets\n",
    "\n",
    "# Create Face Embeddings\n",
    "\n",
    "Download FaceNet implementation from: https://drive.google.com/open?id=1pwQ3H4aJ8a6yyJHZkTwtjcL4wYWQb7bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a face embedding for each face in the dataset using facenet\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from keras.models import load_model\n",
    " \n",
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "\t# scale pixel values\n",
    "\tface_pixels = face_pixels.astype('float32')\n",
    "\t# standardize pixel values across channels (global)\n",
    "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
    "\tface_pixels = (face_pixels - mean) / std\n",
    "\t# transform face into one sample\n",
    "\tsamples = expand_dims(face_pixels, axis=0)\n",
    "\t# make prediction to get embedding\n",
    "\tyhat = model.predict(samples)\n",
    "\treturn yhat[0]\n",
    " \n",
    "# load the face dataset\n",
    "data = load('5-celebrity-faces-dataset.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\n",
    "# load the facenet model\n",
    "model = load_model('facenet_keras.h5')\n",
    "print('Loaded Model')\n",
    "\n",
    "# convert each face in the train set to an embedding\n",
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTrainX.append(embedding)\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)\n",
    "\n",
    "# convert each face in the test set to an embedding\n",
    "newTestX = list()\n",
    "for face_pixels in testX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTestX.append(embedding)\n",
    "newTestX = asarray(newTestX)\n",
    "print(newTestX.shape)\n",
    "\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('5-celebrity-faces-embeddings.npz', newTrainX, trainy, newTestX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs formation and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
    "\n",
    "# 1) Load dataset\n",
    "data = np.load('5-celebrity-faces-dataset.npz')\n",
    "trainX, trainy, testX, testy = data[\"arr_0\"], data[\"arr_1\"], data[\"arr_2\"], data[\"arr_3\"]\n",
    "print(\"Dataset: train=%d, test=%d\" % (trainX.shape[0], testX.shape[0]))\n",
    "\n",
    "# 2) Normalize input vectors\n",
    "in_encoder = Normalizer(norm = \"l2\")\n",
    "#print(trainX[0]) # Embedding without normalization\n",
    "normed_trainX = in_encoder.transform(trainX)\n",
    "#print(trainX[0]) # Embedding with normalization\n",
    "normed_testX = in_encoder.transform(testX)\n",
    "\n",
    "# 3) Label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "print(trainy)\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "print(trainy)\n",
    "testy = out_encoder.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding with it's own photo\n",
    "\n",
    "count = 0\n",
    "trainX2 = list()\n",
    "for face in normed_trainX:\n",
    "    new_emb = face\n",
    "    new_emb = np.append(new_emb, count)\n",
    "    #print(trainy[count])\n",
    "    #print(new_emb)\n",
    "    trainX2.append(new_emb)\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "testX2 = list()\n",
    "for face in normed_testX:\n",
    "    new_emb = face\n",
    "    new_emb = np.append(new_emb, count)\n",
    "    #print(trainy[count])\n",
    "    #print(new_emb)\n",
    "    testX2.append(new_emb)\n",
    "    count += 1 \n",
    "\n",
    "# new_emb = [0.42, -0.11, ..., 0], [1.22, -2.11, ..., 24] \n",
    "# OBS: - the last number in the array (new_emb) is the index of the img in their raw dataset (the img dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_obtention(trainX2):\n",
    "    how_many = len(trainX2) # how_many = 93\n",
    "\n",
    "    # Formation of pairs (everyone with everyone)\n",
    "\n",
    "    pairs = list() \n",
    "    # pairs = [ [img_i, img_j, label_i, label_j]   ] - img_i is the embedding of i\n",
    "\n",
    "    for i in range(how_many):\n",
    "        for j in range(i+1, how_many):\n",
    "            pair = list()\n",
    "            #print(\"(\" + str(i) + \",\" + str(j) + \")\")\n",
    "            img_i = trainX2[i][0:-1]\n",
    "            label_i = trainX2[i][-1]\n",
    "            img_j = trainX2[j][0:-1]\n",
    "            label_j = trainX2[j][-1]\n",
    "            pair.append(img_i)\n",
    "            pair.append(img_j)\n",
    "            pair.append(label_i)\n",
    "            pair.append(label_j)\n",
    "            pairs.append(pair)\n",
    "\n",
    "    # 2) Pair comparison\n",
    "\n",
    "    \n",
    "    threshold = np.linspace(0.1, 0.9, num=9, endpoint=True, retstep=False, dtype=float, axis=0)\n",
    "    Psame = list()\n",
    "    Pdiff = list()\n",
    "    for t in range(len(threshold)):\n",
    "        #print(\"Threshold: \" + str(threshold[t]))\n",
    "        for pair in pairs:\n",
    "            norm = np.linalg.norm(pair[0] - pair[1])**2\n",
    "            comparison = [pair[0], pair[1], pair[2], pair[3], norm]\n",
    "            if norm < threshold[t]:\n",
    "                Psame.append(comparison)\n",
    "            else:\n",
    "                Pdiff.append(comparison)\n",
    "       \n",
    "    # comparison = [img_i, img_j, label_i, label_j, norm]\n",
    "\n",
    "    print(\"Pairs: \" + str(len(pairs)))\n",
    "    print(\"Psame: \" + str(len(Psame)))\n",
    "    print(\"Pdiff: \" + str(len(Pdiff)))\n",
    "\n",
    "    # 3) VAL and FAR rates\n",
    "\n",
    "    VAL_list = []\n",
    "    FAR_list = []\n",
    "    TA_total = []\n",
    "    FA_total = []\n",
    "    for t in range(len(threshold)):\n",
    "        TA = [] \n",
    "        FA = []\n",
    "        fa_counter = 0\n",
    "        ta_counter = 0\n",
    "        for pair in Psame:\n",
    "            if pair[-1] <= threshold[t]:\n",
    "                TA.append(pair)\n",
    "                ta_counter = len(TA)\n",
    "                #print(\"TA para threshold:\" + str(threshold[t]) + \"es\" + str(ta_counter))\n",
    "                TA_total.append(TA)\n",
    "        for pair in Pdiff:\n",
    "            if pair[-1] <= threshold[t]:\n",
    "                FA.append(pair)\n",
    "                fa_counter = len(FA)\n",
    "                #print(\"FA para threshold:\" + str(threshold[t]) + \"es\" + str(fa_counter))\n",
    "                TA_total.append(TA)\n",
    "        if len(Psame) != 0:\n",
    "            val = ta_counter/len(Psame)\n",
    "        else:\n",
    "            val = 0\n",
    "        print(\"VAL para threshold:\" + str(threshold[t]) + \"es\" + str(val))\n",
    "        VAL_list.append(val)\n",
    "        #print(VAL)\n",
    "        if len(Pdiff) != 0:\n",
    "            far = fa_counter/len(Pdiff)\n",
    "        else:\n",
    "            far = 0\n",
    "        print(\"FAR para threshold:\" + str(threshold[t]) + \"es\" + str(far))\n",
    "        FAR_list.append(far)\n",
    "\n",
    "    metrics = [pairs, Psame, Pdiff, TA_total, FA_total, VAL_list, FAR_list]\n",
    "    return metrics\n",
    "\n",
    "def face_pairs_plot(metrics_data, raw_imgs, how_many):\n",
    "    for i in range(how_many):\n",
    "        plt.figure()\n",
    "        f, axarr = plt.subplots(1,2)\n",
    "        axarr[0].imshow(raw_imgs[int(metrics_data[1][i][2])])\n",
    "        axarr[1].imshow(raw_imgs[int(metrics_data[1][i][3])]) \n",
    "        # [1][i][2] means:\n",
    "        # 1: Access Psame\n",
    "        # i: Access the i-th term of Psame pairs\n",
    "        # 2: Access the 2nd term of a single pair of Psame (which is the label of the img in the raw_imgs dataset)\n",
    "    return\n",
    "\n",
    "# Plot Roc curve\n",
    "def ROC_plot(VAL, FAR):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    #ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.grid(linestyle = '-', color = 'gray')\n",
    "    plt.xticks(fontsize=14,fontweight='normal')\n",
    "    plt.yticks(fontsize=14,fontweight='normal')\n",
    "    plt.xlabel('FAR', fontsize=14)\n",
    "    plt.ylabel('VAL', fontsize=14)\n",
    "    plt.xlim(0,0.015)\n",
    "    plt.ylim(0,1)\n",
    "    #ax.invert_xaxis()\n",
    "    #ax.plot(FAR_list, VAL_list, 'g-', linewidth = 1.5)\n",
    "    ax.plot(FAR, VAL, 'g-', linewidth = 1.5)\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Results are stored in repo: https://github.com/JoseLGP/FaceRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
